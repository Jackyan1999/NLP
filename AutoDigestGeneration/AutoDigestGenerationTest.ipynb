{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4246\n",
      "4246\n"
     ]
    }
   ],
   "source": [
    "data_file_path=r'D:\\NLP\\github\\中文新闻\\cnews.train.txt'\n",
    "with open(data_file_path,'r',encoding='utf-8') as f:\n",
    "    lines=f.readlines()\n",
    "\n",
    "titles=[]\n",
    "texts=[]\n",
    "for news in lines[:10000]:\n",
    "    news=news[3:]\n",
    "    if('新浪体育讯' in news):\n",
    "        title=news.split('新浪体育讯')[0]\n",
    "        text=''.join(news.split('新浪体育讯')[1:])\n",
    "        titles.append(title.strip())\n",
    "        texts.append(text.strip())\n",
    "print(len(titles))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBA最性感女球迷裸照 手拉胸衣露薄纱浑圆半球(图)\n",
      "尽管拜伦-戴维斯至今还处在伤病当中，但他距离复出的日子已经不远了。据洛杉矶媒体的消息，戴维斯现在已经恢复了和全队的合练，并且将在下周重新回到赛场。到时候，人们又能看到场边那个性感女神一般的女人为他在现场加油助威了。戴维斯和杰西卡还有他的丈夫都是非常要好的朋友，杰西卡一有空都会去给戴维斯和他的球队捧场，她也很快成为了戴维斯的钟情粉丝。因此，她也是NBA中除了帕克的妻子伊娃之外，知名度最高的女球迷。杰西卡-阿尔芭年轻时就拥有很高的表演天赋，她13岁开始拍片，2005年通过主演《罪恶之城》、《神奇四侠》达到事业顶峰。虽然大多以青春靓丽的“花瓶”形象出现，但她的美丽身影通常是影片中最迷人的风景。连续三年入选了《男人帮》性感女星评选的前十名，是美国“最性感的女星”之一。有球迷甚至把她称作是甜美的巧克力美人，她1981年4月28日出生于加利福尼亚的波莫纳市，拥有5国混血血统。妈妈拥有加拿大、法国、和丹麦人的混血，爸爸是墨西哥和美国的混血儿。这就解释了杰西卡棕色的肌肤和漂亮的深棕色眼睛。金牛座的女人天生就是一个精力充沛，生活欲望强烈且楚楚动人的女生。美国的著名演员杰西卡-阿尔芭就是这样的，她很会按照女人的特点无忧无虑地生活。她魅力无限，渴望经历爱情生活的全过程：恋爱、结婚、家庭、孩子和美味佳肴对她来说都是生活中不可错过的事情。令人一直感到意外的是身在好莱坞这个大染缸，杰西卡依然能坚守住自己的那片处女地。她曾连续三年被评为“全球最性感的女人”前五名，可是她享受性爱却拒绝裸戏。她甚至把《花花公子》告上法庭，原因是《花花公子》在没有征求她允许的情况下擅自“盗用”了她的比基尼剧照作了杂志封面。虽然一直在娱乐圈里留下了圣洁的名声，但最近网站上曝光的一组露点照片却让杰西卡-阿尔芭陷入了窘境。在美国网站《egotastic》曝光的这组照片中，有多张杰西卡-阿尔芭大着肚子的露点照片，这让人们完全改变了以往她在人们心中留下的印象。当初和花花公子闹上公堂的时候，杰西卡曾经对媒体说：“你可能不会相信，我拒绝裸戏！在众人面前不穿衣服我会不知所措！登上《花花公子》封面这会给很多人误解，以为里面会有我的裸照。”可现在，她的半裸照片却清晰的被挂在了网站上。只是不知道戴维斯看了这些照片之后会作何感想。(乳娃娃)\n"
     ]
    }
   ],
   "source": [
    "print(titles[1000])\n",
    "print(texts[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_len_avg:23.757654\n",
      "title_len_middle:23.000000\n",
      "title_len_min:13.000000\n",
      "title_len_max:30.000000\n",
      "text_len_avg:838.758832\n",
      "text_len_middle:813.000000\n",
      "text_len_min:48.000000\n",
      "text_len_max:12395.000000\n"
     ]
    }
   ],
   "source": [
    "titles_lens=[]\n",
    "texts_lens=[]\n",
    "for line in titles:\n",
    "    titles_lens.append(len(line))\n",
    "for line in texts:\n",
    "    texts_lens.append(len(line))\n",
    "titles_lens.sort()\n",
    "print('title_len_avg:%f'%(sum(titles_lens)/len(titles)))\n",
    "print('title_len_middle:%f'%(titles_lens[int(len(titles)/2)]))\n",
    "print('title_len_min:%f'%(titles_lens[0]))\n",
    "print('title_len_max:%f'%(titles_lens[len(titles)-1]))\n",
    "texts_lens.sort()\n",
    "print('text_len_avg:%f'%(sum(texts_lens)/len(texts)))\n",
    "print('text_len_middle:%f'%(texts_lens[int(len(texts)/2)]))\n",
    "print('text_len_min:%f'%(texts_lens[0]))\n",
    "print('text_len_max:%f'%(texts_lens[len(texts)-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense,Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1.M6S\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.761 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46705\n",
      "46701\n"
     ]
    }
   ],
   "source": [
    "MAX_WORDS=10000\n",
    "\n",
    "#将句子分词并用空格隔开\n",
    "inputTextList=[' '.join([w for w in jieba.cut(text)]) for text in texts]\n",
    "targetTextList=[' '.join([w for w in jieba.cut(text)]) for text in titles]\n",
    "\n",
    "#-4的原因是后面会加上4个特殊字符串，值从1开始，故词典里只设置9999个词\n",
    "tokenizer=Tokenizer(num_words=MAX_WORDS-5)\n",
    "tokenizer.fit_on_texts(texts=inputTextList+targetTextList)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "# 增加特殊编码\n",
    "SPECIAL_CODES = ['<PAD>', '<EOS>', '<UNK>', '<GO>']\n",
    "\n",
    "for i,w in enumerate(SPECIAL_CODES):\n",
    "    word_index[w]=MAX_WORDS-i-1\n",
    "\n",
    "re_word_index = dict([(i, t) for t, i in word_index.items()])\n",
    "\n",
    "with open('AutoDigestGeneration_Dict.txt','w',encoding='utf-8') as f:\n",
    "    f.write(str(word_index))\n",
    "\n",
    "#将文本映射为数字序列\n",
    "input_sequences=tokenizer.texts_to_sequences(texts=inputTextList)\n",
    "target_sequences=tokenizer.texts_to_sequences(texts=targetTextList)\n",
    "\n",
    "print(len(word_index))\n",
    "print(len(re_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[372, 2461, 735, 1207, 38, 2543, 398, 1555, 1, 27, 6, 852, 290, 2, 2085, 51, 4916, 5, 3, 317, 387, 444, 2, 133, 1, 735, 106, 51, 670, 5, 11, 371, 2, 4303, 1, 496, 33, 4, 3936, 516, 397, 1107, 3, 4316, 1, 710, 94, 67, 348, 1199, 642, 4477, 7280, 816, 2, 3703, 104, 6, 4, 588, 3166, 7360, 5, 3, 735, 11, 8336, 135, 6, 2, 9755, 20, 7, 132, 93, 107, 2, 1797, 1, 8336, 251, 20, 61, 178, 118, 735, 11, 6, 2, 19, 8776, 1, 1138, 10, 733, 165, 5, 735, 2, 3695, 3, 501, 1, 1138, 10, 7, 128, 14, 505, 680, 2, 3085, 2372, 922, 1, 441, 2, 3, 8336, 7739, 538, 64, 24, 519, 43, 556, 2, 858, 1090, 1, 1138, 141, 531, 112, 1, 1825, 102, 652, 7251, 374, 578, 1965, 383, 21, 374, 383, 426, 6678, 3, 145, 4118, 60, 5546, 2, 12, 13, 3851, 302, 1, 27, 1138, 2, 3693, 2672, 7, 14, 152, 2, 9424, 3, 167, 1721, 1349, 5, 374, 383, 7763, 4252, 2, 1, 7, 609, 12, 152, 4477, 2, 13, 535, 3, 18, 154, 335, 164, 1138, 7436, 7, 2, 1, 1138, 8031, 102, 53, 37, 410, 7466, 2, 1, 519, 56, 3, 4338, 519, 5388, 21, 4191, 21, 11, 58, 2, 1, 7740, 7, 11, 609, 2, 3, 23, 24, 1418, 5, 8336, 2, 11, 1761, 2, 3891, 3, 2, 3703, 4656, 97, 42, 1, 2043, 2992, 3828, 2017, 2, 3, 609, 2, 8336, 7739, 97, 105, 2, 1, 1138, 1453, 3703, 2, 2156, 281, 2043, 3, 1138, 5706, 3471, 1, 1716, 808, 8679, 2043, 2, 16, 21, 6453, 21, 4038, 21, 1625, 11, 55, 1138, 211, 20, 7, 2043, 14, 2109, 2005, 2, 404, 3, 894, 191, 799, 1306, 2, 7, 2364, 4, 7906, 116, 1, 8336, 369, 67, 1331, 52, 2, 3, 1138, 378, 167, 1721, 48, 6866, 12, 4725, 152, 4477, 2, 3703, 13, 109, 6263, 1, 574, 1138, 2306, 8342, 136, 1714, 3, 1138, 335, 164, 374, 383, 1, 432, 7, 374, 383, 4, 34, 1138, 3061, 2, 204, 89, 12, 13, 5, 1138, 2, 3799, 5, 6092, 3, 145, 191, 4, 9831, 177, 1005, 5, 2, 8252, 1, 27, 279, 2320, 35, 3771, 2, 5755, 2641, 136, 41, 8336, 7739, 1094, 5, 4484, 3, 4, 609, 2320, 374, 383, 3771, 2, 8853, 2641, 14, 1, 8336, 7739, 2, 2641, 1, 23, 41, 710, 384, 703, 5, 1387, 1138, 4, 710, 2116, 1005, 2, 1675, 3, 1808, 11, 3959, 35, 2, 117, 1, 8336, 476, 55, 444, 46, 16, 12, 95, 163, 227, 664, 1, 15, 1714, 234, 4, 3908, 810, 31, 1101, 8388, 234, 4895, 374, 383, 2455, 118, 246, 58, 1, 1983, 2118, 61, 18, 15, 2, 3, 13, 406, 106, 1, 1138, 2, 2641, 136, 4029, 2, 48, 3601, 4, 5, 2320, 35, 3, 255, 31, 183, 735, 331, 5, 364, 2641, 88, 61, 8677, 7214, 3, 3238, 2178]\n",
      "尽管拜伦戴维斯至今还处在伤病当中，但他距离复出的日子已经不远了。据洛杉矶媒体的消息，戴维斯现在已经恢复了和全队的合练，并且将在下周重新回到赛场。到时候，人们又能看到场边那个性感女神一般的女人为他在现场加油助威了。戴维斯和杰西卡还有他的丈夫都是非常要好的朋友，杰西卡一都会去给戴维斯和他的球队捧场，她也很快成为了戴维斯的粉丝。因此，她也是nba中除了帕克的妻子伊娃之外，最高的。杰西卡阿尔年轻时就拥有很高的表演天赋，她13岁开始，2005年通过主演《之城》、《》达到事业。虽然大多以青春的“”形象出现，但她的身影通常是中最的风景。连续三年入选了《》星评选的，是美国“最性感的”之一。有球迷甚至把她称作是的，她1981年4月28生于的，拥有5。妈妈拥有加拿大、法国、和人的，爸爸是和美国的。这就解释了杰西卡的和漂亮的眼睛。的女人天生就是一个，生活欲望强烈且的。美国的杰西卡阿尔就是这样的，她按照女人的特点地生活。她魅力无限，渴望经历爱情生活的：、结婚、家庭、孩子和对她来说都是生活中不可错过的事情。令人一直感到意外的是身在好莱坞这个，杰西卡依然能住自己的。她曾连续三年被评为“全球最性感的女人”前五名，可是她享受性爱却拒绝。她甚至把《》，原因是《》在没有她允许的情况下“”了她的作了杂志。虽然一直在娱乐圈里留下了的名声，但最近网站上曝光的一组照片却让杰西卡阿尔陷入了窘境。在美国网站《》曝光的这组照片中，杰西卡阿尔的照片，这让人们完全改变了以往她在人们心中留下的印象。当初和闹上的时候，杰西卡曾经对媒体说：“你可能不会相信，我拒绝！在众人面前不我会不知所措！登上《》这会给很多人，以为里面会有我的。”可现在，她的照片却清晰的被挂在了网站上。只是不知道戴维斯看了这些照片之后会作何感想。乳娃娃"
     ]
    }
   ],
   "source": [
    "print(input_sequences[1000])\n",
    "\n",
    "for n in input_sequences[1000]:\n",
    "    print(re_word_index[n],end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_len_avg:478.837965\n",
      "title_len_middle:465.000000\n",
      "title_len_min:26.000000\n",
      "title_len_max:6457.000000\n",
      "text_len_avg:10.961611\n",
      "text_len_middle:11.000000\n",
      "text_len_min:3.000000\n",
      "text_len_max:19.000000\n"
     ]
    }
   ],
   "source": [
    "titles_lens=[]\n",
    "texts_lens=[]\n",
    "for line in input_sequences:\n",
    "    titles_lens.append(len(line))\n",
    "for line in target_sequences:\n",
    "    texts_lens.append(len(line))\n",
    "titles_lens.sort()\n",
    "print('title_len_avg:%f'%(sum(titles_lens)/len(titles)))\n",
    "print('title_len_middle:%f'%(titles_lens[int(len(titles)/2)]))\n",
    "print('title_len_min:%f'%(titles_lens[0]))\n",
    "print('title_len_max:%f'%(titles_lens[len(titles)-1]))\n",
    "texts_lens.sort()\n",
    "print('text_len_avg:%f'%(sum(texts_lens)/len(texts)))\n",
    "print('text_len_middle:%f'%(texts_lens[int(len(texts)/2)]))\n",
    "print('text_len_min:%f'%(texts_lens[0]))\n",
    "print('text_len_max:%f'%(texts_lens[len(texts)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#因此确定\n",
    "Tx=500\n",
    "Ty=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 4246/4246 [00:00<00:00, 117267.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 4246/4246 [00:00<00:00, 428060.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4246, 500)\n",
      "(4246, 20)\n"
     ]
    }
   ],
   "source": [
    "#padding数据\n",
    "import tqdm\n",
    "\n",
    "input_arr=[]\n",
    "target_arr=[]\n",
    "\n",
    "for line in tqdm.tqdm(input_sequences):\n",
    "    slen=len(line)\n",
    "    if(slen<Tx):\n",
    "        newline=line+[word_index['<PAD>']]*(Tx-slen)\n",
    "        input_arr.append(newline)\n",
    "    else:\n",
    "        input_arr.append(line[:Tx])\n",
    "\n",
    "\n",
    "for line in tqdm.tqdm(target_sequences):\n",
    "    slen=len(line)\n",
    "    if(slen<Ty):\n",
    "        line.append(word_index['<EOS>'])\n",
    "        newline=line+[word_index['<PAD>']]*(Ty-slen-1)\n",
    "        target_arr.append(newline)\n",
    "    else:\n",
    "        target_arr.append(line[:Ty])\n",
    "\n",
    "input_arr=np.array(input_arr)\n",
    "target_arr=np.array(target_arr)\n",
    "\n",
    "print(input_arr.shape)\n",
    "print(target_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 372 2461  735 1207   38 2543  398 1555    1   27    6  852  290    2\n",
      " 2085   51 4916    5    3  317  387  444    2  133    1  735  106   51\n",
      "  670    5   11  371    2 4303    1  496   33    4 3936  516  397 1107\n",
      "    3 4316    1  710   94   67  348 1199  642 4477 7280  816    2 3703\n",
      "  104    6    4  588 3166 7360    5    3  735   11 8336  135    6    2\n",
      " 9755   20    7  132   93  107    2 1797    1 8336  251   20   61  178\n",
      "  118  735   11    6    2   19 8776    1 1138   10  733  165    5  735\n",
      "    2 3695    3  501    1 1138   10    7  128   14  505  680    2 3085\n",
      " 2372  922    1  441    2    3 8336 7739  538   64   24  519   43  556\n",
      "    2  858 1090    1 1138  141  531  112    1 1825  102  652 7251  374\n",
      "  578 1965  383   21  374  383  426 6678    3  145 4118   60 5546    2\n",
      "   12   13 3851  302    1   27 1138    2 3693 2672    7   14  152    2\n",
      " 9424    3  167 1721 1349    5  374  383 7763 4252    2    1    7  609\n",
      "   12  152 4477    2   13  535    3   18  154  335  164 1138 7436    7\n",
      "    2    1 1138 8031  102   53   37  410 7466    2    1  519   56    3\n",
      " 4338  519 5388   21 4191   21   11   58    2    1 7740    7   11  609\n",
      "    2    3   23   24 1418    5 8336    2   11 1761    2 3891    3    2\n",
      " 3703 4656   97   42    1 2043 2992 3828 2017    2    3  609    2 8336\n",
      " 7739   97  105    2    1 1138 1453 3703    2 2156  281 2043    3 1138\n",
      " 5706 3471    1 1716  808 8679 2043    2   16   21 6453   21 4038   21\n",
      " 1625   11   55 1138  211   20    7 2043   14 2109 2005    2  404    3\n",
      "  894  191  799 1306    2    7 2364    4 7906  116    1 8336  369   67\n",
      " 1331   52    2    3 1138  378  167 1721   48 6866   12 4725  152 4477\n",
      "    2 3703   13  109 6263    1  574 1138 2306 8342  136 1714    3 1138\n",
      "  335  164  374  383    1  432    7  374  383    4   34 1138 3061    2\n",
      "  204   89   12   13    5 1138    2 3799    5 6092    3  145  191    4\n",
      " 9831  177 1005    5    2 8252    1   27  279 2320   35 3771    2 5755\n",
      " 2641  136   41 8336 7739 1094    5 4484    3    4  609 2320  374  383\n",
      " 3771    2 8853 2641   14    1 8336 7739    2 2641    1   23   41  710\n",
      "  384  703    5 1387 1138    4  710 2116 1005    2 1675    3 1808   11\n",
      " 3959   35    2  117    1 8336  476   55  444   46   16   12   95  163\n",
      "  227  664    1   15 1714  234    4 3908  810   31 1101 8388  234 4895\n",
      "  374  383 2455  118  246   58    1 1983 2118   61   18   15    2    3\n",
      "   13  406  106    1 1138    2 2641  136 4029    2   48 3601    4    5\n",
      " 2320   35    3  255   31  183  735  331    5  364 2641   88   61 8677\n",
      " 7214    3 3238 2178 9999 9999 9999 9999 9999 9999]\n",
      "[ 128  152  154 1507 1102 9385 1734 9998 9999 9999 9999 9999 9999 9999\n",
      " 9999 9999 9999 9999 9999 9999]\n"
     ]
    }
   ],
   "source": [
    "print(input_arr[1000])\n",
    "print(target_arr[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义softmax函数\n",
    "def softmax(x, axis=1):\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "\n",
    "# 定义全局网络层对象\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor_tanh = Dense(32, activation = \"tanh\")\n",
    "densor_relu = Dense(1, activation = \"relu\")\n",
    "densor_con = Dense(256, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights')\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    # 将s_prev复制Tx次\n",
    "    s_prev = repeator(s_prev)\n",
    "    # 拼接BiRNN隐层状态与s_prev\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # 计算energies\n",
    "    e = densor_tanh(concat)\n",
    "    energies = densor_relu(e)\n",
    "    # 计算weights\n",
    "    alphas = activator(energies)\n",
    "    # 加权得到Context Vector\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练好的glove词向量\n",
    "with open(r'D:\\NLP\\wordvector\\sgns.zhihu.word\\sgns.zhihu.word', 'r',encoding='utf-8') as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        curr_word = line[0]\n",
    "        words.add(curr_word)\n",
    "        word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_index):\n",
    "    vocab_len = len(word_index) + 1        # Keras Embedding的API要求+1\n",
    "    emb_dim = 300\n",
    "    # 初始化embedding矩阵\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    # 用词向量填充embedding矩阵\n",
    "    for word, index in word_index.items():\n",
    "        word_vector = word_to_vec_map.get(word, np.zeros(emb_dim))\n",
    "        emb_matrix[index, :] = word_vector\n",
    "    # 定义Embedding层，并指定不需要训练该层的权重\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    # build\n",
    "    embedding_layer.build((None,))\n",
    "    # set weights\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# 获取Embedding layer\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def train_gen(X,Y,Ty,n_s,batch_size=64):\n",
    "    xlen=X.shape[0]\n",
    "    permutation = np.random.permutation(xlen)\n",
    "    x = X[permutation]\n",
    "    y = Y[permutation]\n",
    "    num_batches = int(xlen/batch_size)\n",
    "    \n",
    "    while 1:\n",
    "        for i in range(num_batches):\n",
    "            x_batch=x[i*batch_size:(i+1)*batch_size]\n",
    "            y_batch=y[i*batch_size:(i+1)*batch_size]\n",
    "            s0=np.zeros((batch_size,n_s))\n",
    "            c0=np.zeros((batch_size,n_s))\n",
    "            out0=np.zeros((batch_size,MAX_WORDS))#加一的原因是字典中的值从1开始\n",
    "            outputs=np.zeros((batch_size,Ty,MAX_WORDS))\n",
    "            for i,line in enumerate(y_batch):\n",
    "                for j,index in enumerate(line):\n",
    "                    outputs[i,j,index]=1\n",
    "                    \n",
    "            #outputs=np.array(list(map(lambda x: to_categorical(x, num_classes=MAX_WORDS), y_batch)))\n",
    "            outputs = list(outputs.swapaxes(0,1))\n",
    "            yield [x_batch,s0,c0,out0],outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 128 # The hidden size of Bi-LSTM\n",
    "n_s = 128 # The hidden size of LSTM in Decoder\n",
    "decoder_LSTM_cell = LSTM(n_s, return_state=True)\n",
    "output_layer = Dense(MAX_WORDS, activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络层对象（用在model函数中）\n",
    "reshapor = Reshape((1, MAX_WORDS))\n",
    "concator = Concatenate(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_encoder, n_decoder):\n",
    "    \"\"\"\n",
    "    构造模型\n",
    "    @param Tx: 输入序列的长度\n",
    "    @param Ty: 输出序列的长度\n",
    "    @param n_encoder: Encoder端Bi-LSTM隐层结点数\n",
    "    @param n_decoder: Decoder端LSTM隐层结点数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 定义输入层\n",
    "    X = Input(shape=(Tx,))\n",
    "    # Embedding层\n",
    "    embed = embedding_layer(X)\n",
    "    \n",
    "    # 定义Bi-LSTM\n",
    "    a = Bidirectional(LSTM(n_decoder, return_sequences=True))(embed)\n",
    "    \n",
    "    # Decoder端LSTM的初始状态\n",
    "    s0 = Input(shape=(n_decoder,), name='s0')\n",
    "    c0 = Input(shape=(n_decoder,), name='c0')\n",
    "    \n",
    "    # Decoder端LSTM的初始输入\n",
    "    out0 = Input(shape=(MAX_WORDS, ), name='out0')\n",
    "    out = reshapor(out0)\n",
    "    \n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # 模型输出列表，用来存储翻译的结果\n",
    "    outputs = []\n",
    "    \n",
    "    # Decoder端，迭代Ty轮，每轮生成一个翻译结果\n",
    "    for t in range(Ty):\n",
    "        # 获取Context Vector\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # 将Context Vector与上一轮的翻译结果进行concat\n",
    "        con_out=densor_con(reshapor(out))\n",
    "        context = concator([context,con_out])\n",
    "        s, _, c = decoder_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # 将LSTM的输出结果与全连接层链接\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # 存储输出结果\n",
    "        outputs.append(out)\n",
    "    \n",
    "    model = Model([X, s0, c0, out0], outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(Tx, Ty, n_a, n_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model,to_file='attentionTest_model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "'''\n",
    "out = model.compile(optimizer=Adam(lr=0.1, beta_1=0.9, beta_2=0.999, decay=0.001),\n",
    "                    metrics=['accuracy'],\n",
    "                    loss='categorical_crossentropy')\n",
    "'''\n",
    "out = model.compile(optimizer='rmsprop',\n",
    "                    metrics=['accuracy'],\n",
    "                    loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "xlen=len(input_arr)\n",
    "\n",
    "model.fit_generator(train_gen(input_arr,target_arr,Ty,n_s,batch_size=8),\n",
    "                   steps_per_epoch=int(xlen/4),\n",
    "                   epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('AutoDigestGenerationTest.h5')\n",
    "model.save_weights('AutoDigestGenerationTest_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model=load_model('AutoDigestGenerationTest.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "with open('AutoDigestGeneration_Dict.txt','r',encoding='utf-8') as f:\n",
    "    word_index_str=f.read()\n",
    "    \n",
    "\n",
    "    \n",
    "word_index=eval(word_index_str)\n",
    "re_word_index = dict([(i, t) for t, i in word_index.items()])\n",
    "Tx=500\n",
    "n_decoder=128\n",
    "\n",
    "def make_prediction(sentence):\n",
    "    # 将句子分词后转化为数字编码\n",
    "    input_seq=[]\n",
    "    for w in jieba.cut(sentence):\n",
    "        if w in word_index:\n",
    "            input_seq.append(word_index[w])\n",
    "            \n",
    "    if(len(input_seq)<=Tx):\n",
    "        input_seq = np.array(input_seq + [word_index['PAD']] * (Tx - len(input_seq)))\n",
    "    else:\n",
    "        input_seq = np.array(input_seq[:Tx])\n",
    "    s0=np.zeros((1,n_decoder))\n",
    "    c0=np.zeros((1,n_decoder))\n",
    "    out0=np.zeros((1,MAX_WORDS))\n",
    "    \n",
    "    print(input_seq)\n",
    "    \n",
    "    # 翻译结果\n",
    "    preds = model.predict([input_seq.reshape(-1,Tx), s0, c0, out0])\n",
    "    predictions = np.argmax(preds, axis=-1)\n",
    "    \n",
    "    print(predictions)\n",
    "    \n",
    "    # 转换为单词\n",
    "    idx = [re_word_index.get(idx[0], \"<UNK>\") for idx in predictions]\n",
    "    \n",
    "    # 返回句子\n",
    "    return \" \".join(idx)\n",
    "\n",
    "text='''\n",
    "尽管拜伦-戴维斯至今还处在伤病当中，但他距离复出的日子已经不远了。据洛杉矶媒体的消息，戴维斯现在已经恢复了和全队的合练，并且将在下周重新回到赛场。到时候，人们又能看到场边那个性感女神一般的女人为他在现场加油助威了。戴维斯和杰西卡还有他的丈夫都是非常要好的朋友，杰西卡一有空都会去给戴维斯和他的球队捧场，她也很快成为了戴维斯的钟情粉丝。因此，她也是NBA中除了帕克的妻子伊娃之外，知名度最高的女球迷。杰西卡-阿尔芭年轻时就拥有很高的表演天赋，她13岁开始拍片，2005年通过主演《罪恶之城》、《神奇四侠》达到事业顶峰。虽然大多以青春靓丽的“花瓶”形象出现，但她的美丽身影通常是影片中最迷人的风景。连续三年入选了《男人帮》性感女星评选的前十名，是美国“最性感的女星”之一。有球迷甚至把她称作是甜美的巧克力美人，她1981年4月28日出生于加利福尼亚的波莫纳市，拥有5国混血血统。妈妈拥有加拿大、法国、和丹麦人的混血，爸爸是墨西哥和美国的混血儿。这就解释了杰西卡棕色的肌肤和漂亮的深棕色眼睛。金牛座的女人天生就是一个精力充沛，生活欲望强烈且楚楚动人的女生。美国的著名演员杰西卡-阿尔芭就是这样的，她很会按照女人的特点无忧无虑地生活。她魅力无限，渴望经历爱情生活的全过程：恋爱、结婚、家庭、孩子和美味佳肴对她来说都是生活中不可错过的事情。令人一直感到意外的是身在好莱坞这个大染缸，杰西卡依然能坚守住自己的那片处女地。她曾连续三年被评为“全球最性感的女人”前五名，可是她享受性爱却拒绝裸戏。她甚至把《花花公子》告上法庭，原因是《花花公子》在没有征求她允许的情况下擅自“盗用”了她的比基尼剧照作了杂志封面。虽然一直在娱乐圈里留下了圣洁的名声，但最近网站上曝光的一组露点照片却让杰西卡-阿尔芭陷入了窘境。在美国网站《egotastic》曝光的这组照片中，有多张杰西卡-阿尔芭大着肚子的露点照片，这让人们完全改变了以往她在人们心中留下的印象。当初和花花公子闹上公堂的时候，杰西卡曾经对媒体说：“你可能不会相信，我拒绝裸戏！在众人面前不穿衣服我会不知所措！登上《花花公子》封面这会给很多人误解，以为里面会有我的裸照。”可现在，她的半裸照片却清晰的被挂在了网站上。只是不知道戴维斯看了这些照片之后会作何感想。(乳娃娃)\n",
    "'''\n",
    "print(text)\n",
    "result=make_prediction(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
